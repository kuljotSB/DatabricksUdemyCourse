{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dab8721-85fa-461d-8417-918fcee8cc08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Agent notebook\n",
    "\n",
    "This is an auto-generated notebook created by an AI Playground export. We generated three notebooks in the same folder:\n",
    "- [**agent**]($./agent): contains the code to build the agent.\n",
    "- [config.yml]($./config.yml): contains the configurations.\n",
    "- [driver]($./driver): logs, evaluate, registers, and deploys the agent.\n",
    "\n",
    "This notebook uses Mosaic AI Agent Framework ([AWS](https://docs.databricks.com/en/generative-ai/retrieval-augmented-generation.html) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/retrieval-augmented-generation)) to recreate your agent from the AI Playground. It defines a LangChain agent that has access to the tools from the source Playground session.\n",
    "\n",
    "Use this notebook to iterate on and modify the agent. For example, you could add more tools or change the system prompt.\n",
    "\n",
    " **_NOTE:_**  This notebook uses LangChain, however AI Agent Framework is compatible with other agent frameworks like Pyfunc and LlamaIndex.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Address all `TODO`s in this notebook.\n",
    "- Review the contents of [config.yml]($./config.yml) as it defines the tools available to your agent, the LLM endpoint, and the agent prompt.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "After testing and iterating on your agent in this notebook, go to the auto-generated [driver]($./driver) notebook in this folder to log, register, and deploy the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94cb18c8-7d58-4614-8c19-693409c9b071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -qqqq mlflow-skinny langchain==0.2.16 langgraph-checkpoint==1.0.12 langchain_core langchain-community==0.2.16 langgraph==0.2.16 pydantic databricks_langchain\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e7e3344-265f-47f8-9924-7c58eed68100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1466e6a-25e9-44ff-8432-81c02fd97358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Import and setup\n",
    "\n",
    "Use `mlflow.langchain.autolog()` to set up [MLflow traces](https://docs.databricks.com/en/mlflow/mlflow-tracing.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b6ff237-e7a6-4617-8501-7ea7fe251bf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import ModelConfig\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "config = ModelConfig(development_config=\"config.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b5502f5-a811-444b-82b1-f245913d285c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define the chat model and tools\n",
    "Create a LangChain chat model that supports [LangGraph tool](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/) calling.\n",
    "\n",
    "Modify the tools your agent has access to by modifying the `uc_functions` list in [config.yml]($./config.yml). Any non-UC function spec tools can be defined in this notebook. See [LangChain - How to create tools](https://python.langchain.com/v0.2/docs/how_to/custom_tools/) and [LangChain - Using built-in tools](https://python.langchain.com/v0.2/docs/how_to/tools_builtin/).\n",
    "\n",
    " **_NOTE:_**  This notebook uses LangChain, however AI Agent Framework is compatible with other agent frameworks like Pyfunc and LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "807a1a1f-f159-4742-9174-dd91281ab25f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatDatabricks\n",
    "from langchain_community.tools.databricks import UCFunctionToolkit\n",
    "import requests\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.tools import Tool\n",
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Create the llm\n",
    "llm = ChatDatabricks(endpoint=config.get(\"llm_endpoint\"))\n",
    "\n",
    "uc_functions = config.get(\"uc_functions\")\n",
    "\n",
    "tools = (\n",
    "    UCFunctionToolkit(warehouse_id=config.get(\"warehouse_id\"))\n",
    "    .include(*uc_functions)\n",
    "    .get_tools()\n",
    ")\n",
    "        \n",
    "def get_weather(location):\n",
    "            \"\"\"\n",
    "            Fetch the weather information for a particular location that the user is interested in.\n",
    "\n",
    "            Parameters:\n",
    "            location (string): the location whose weather information needs to be fetched\n",
    "\n",
    "            Returns:\n",
    "                weather (string): The weather information for the given location\n",
    "            \"\"\"\n",
    "            url = \"http://api.openweathermap.org/geo/1.0/direct?q=\" + location + \"&limit=1&appid=YOUR_API_KEY\"\n",
    "            response=requests.get(url)\n",
    "            get_response=response.json()\n",
    "            latitude=get_response[0]['lat']\n",
    "            longitude = get_response[0]['lon']\n",
    "\n",
    "            url_final = \"https://api.openweathermap.org/data/2.5/weather?lat=\" + str(latitude) + \"&lon=\" + str(longitude) + \"&appid=YOUR_API_KEY\"\n",
    "            final_response = requests.get(url_final)\n",
    "            final_response_json = final_response.json()\n",
    "            weather=final_response_json['weather'][0]['description']\n",
    "            print(weather)\n",
    "            return(weather)\n",
    "        \n",
    "\n",
    "weather_tool = Tool(\n",
    "    name=\"get_weather\",\n",
    "    func=lambda location: get_weather(location),\n",
    "    description=\"Fetches weather information for a specified location. The location should be provided as a string (e.g., 'Mumbai').\"\n",
    ")\n",
    "\n",
    "tools.append(weather_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b22f5015-08e6-42b1-b1b1-9d780fa89e44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Optional**: Using Databricks Vector Search retrieval as a tool in your LangGraph Agent\n",
    "\n",
    "**This notebook section is optional and can be deleted**\n",
    "\n",
    "A common agent use case is Retrieval Augmented Generation (RAG). In RAG, the agent can use a vector search retriever to query a corpus of documents to provide additional context to the LLM. If you already have a Databricks vector search endpoint and index, you can easily create a tool that performs retrieval against the index and passes the results to your agent.\n",
    "\n",
    "**Prerequisite**: You must have an existing Vector Search endpoint and index. For more information on how to create an index, see Databricks documentation ([AWS](https://docs.databricks.com/generative-ai/create-query-vector-search.html) | [Azure](https://learn.microsoft.com/azure/databricks/generative-ai/create-query-vector-search))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4831f6b3-3d4e-4d6e-b44c-837091a4b4d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from langchain.tools.retriever import create_retriever_tool\n",
    "# from databricks_langchain import DatabricksVectorSearch\n",
    "\n",
    "# # Connect to an existing Databricks Vector Search endpoint and index\n",
    "# vector_store = DatabricksVectorSearch(\n",
    "#   endpoint=\"\", # TODO: Fill in with your VS endpoint name\n",
    "#   index_name=\"\", # TODO: Fill in with your VS index name\n",
    "#   columns=[\n",
    "#     \"\"\n",
    "#   ] # TODO: Fill in with column names\n",
    "# ).as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# # Create a tool object that performs retrieval against our vector search index\n",
    "# retriever_tool = create_retriever_tool(\n",
    "#   vector_store,\n",
    "#   name=\"\", # TODO: Fill in your retriever's name\n",
    "#   description=\"\", # TODO: Fill in your retriever's description to help the LLM choose this tool\n",
    "# )\n",
    "\n",
    "# # Specify the return type schema of our retriever, so that evaluation and UIs can\n",
    "# # automatically display retrieved chunks\n",
    "# mlflow.models.set_retriever_schema(\n",
    "#     primary_key=\"\",\n",
    "#     text_column=\"\",\n",
    "#     doc_uri=\"\",\n",
    "#     name=\"\",\n",
    "# )\n",
    "\n",
    "# tools.append(retriever_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8cfa7fd-3309-45f8-b3da-cf0f4c64ec3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Output parsers\n",
    "Databricks interfaces, such as the AI Playground, can optionally display pretty-printed tool calls.\n",
    "\n",
    "Use the following helper functions to parse the LLM's output into the expected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce71c631-11c1-4f89-aef9-2719b33185ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Iterator, Dict, Any\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    ToolMessage,\n",
    "    MessageLikeRepresentation,\n",
    ")\n",
    "\n",
    "import json\n",
    "\n",
    "def stringify_tool_call(tool_call: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Convert a raw tool call into a formatted string that the playground UI expects if there is enough information in the tool_call\n",
    "    \"\"\"\n",
    "    try:\n",
    "        request = json.dumps(\n",
    "            {\n",
    "                \"id\": tool_call.get(\"id\"),\n",
    "                \"name\": tool_call.get(\"name\"),\n",
    "                \"arguments\": json.dumps(tool_call.get(\"args\", {})),\n",
    "            },\n",
    "            indent=2,\n",
    "        )\n",
    "        return f\"<tool_call>{request}</tool_call>\"\n",
    "    except:\n",
    "        return str(tool_call)\n",
    "\n",
    "\n",
    "def stringify_tool_result(tool_msg: ToolMessage) -> str:\n",
    "    \"\"\"\n",
    "    Convert a ToolMessage into a formatted string that the playground UI expects if there is enough information in the ToolMessage\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = json.dumps(\n",
    "            {\"id\": tool_msg.tool_call_id, \"content\": tool_msg.content}, indent=2\n",
    "        )\n",
    "        return f\"<tool_call_result>{result}</tool_call_result>\"\n",
    "    except:\n",
    "        return str(tool_msg)\n",
    "\n",
    "\n",
    "def parse_message(msg) -> str:\n",
    "    \"\"\"Parse different message types into their string representations\"\"\"\n",
    "    # tool call result\n",
    "    if isinstance(msg, ToolMessage):\n",
    "        return stringify_tool_result(msg)\n",
    "    # tool call\n",
    "    elif isinstance(msg, AIMessage) and msg.tool_calls:\n",
    "        tool_call_results = [stringify_tool_call(call) for call in msg.tool_calls]\n",
    "        return \"\".join(tool_call_results)\n",
    "    # normal HumanMessage or AIMessage (reasoning or final answer)\n",
    "    elif isinstance(msg, (AIMessage, HumanMessage)):\n",
    "        return msg.content\n",
    "    else:\n",
    "        print(f\"Unexpected message type: {type(msg)}\")\n",
    "        return str(msg)\n",
    "\n",
    "\n",
    "def wrap_output(stream: Iterator[MessageLikeRepresentation]) -> Iterator[str]:\n",
    "    \"\"\"\n",
    "    Process and yield formatted outputs from the message stream.\n",
    "    The invoke and stream langchain functions produce different output formats.\n",
    "    This function handles both cases.\n",
    "    \"\"\"\n",
    "    for event in stream:\n",
    "        # the agent was called with invoke()\n",
    "        if \"messages\" in event:\n",
    "            for msg in event[\"messages\"]:\n",
    "                yield parse_message(msg) + \"\\n\\n\"\n",
    "        # the agent was called with stream()\n",
    "        else:\n",
    "            for node in event:\n",
    "                for key, messages in event[node].items():\n",
    "                    if isinstance(messages, list):\n",
    "                        for msg in messages:\n",
    "                            yield parse_message(msg) + \"\\n\\n\"\n",
    "                    else:\n",
    "                        print(\"Unexpected value {messages} for key {key}. Expected a list of `MessageLikeRepresentation`'s\")\n",
    "                        yield str(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07f85a16-5732-4837-8640-2cd433cc237d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create the agent\n",
    "Here we provide a simple graph that uses the model and tools defined by [config.yml]($./config.yml). This graph is adapated from [this LangGraph guide](https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch/).\n",
    "\n",
    "\n",
    "To further customize your LangGraph agent, you can refer to:\n",
    "* [LangGraph - Quick Start](https://langchain-ai.github.io/langgraph/tutorials/introduction/) for explanations of the concepts used in this LangGraph agent\n",
    "* [LangGraph - How-to Guides](https://langchain-ai.github.io/langgraph/how-tos/) to expand the functionality of your agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bea59763-4e89-46de-992b-7a3f2ef6b35d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Annotated,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    TypedDict,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "\n",
    "\n",
    "# We create the AgentState that we will pass around\n",
    "# This simply involves a list of messages\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of the agent.\"\"\"\n",
    "\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolExecutor, Sequence[BaseTool]],\n",
    "    agent_prompt: Optional[str] = None,\n",
    ") -> CompiledGraph:\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    # Define the function that determines which node to go to\n",
    "    def should_continue(state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # If there is no function call, then we finish\n",
    "        if not last_message.tool_calls:\n",
    "            return \"end\"\n",
    "        else:\n",
    "            return \"continue\"\n",
    "\n",
    "    if agent_prompt:\n",
    "        system_message = SystemMessage(content=agent_prompt)\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [system_message] + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "    model_runnable = preprocessor | model\n",
    "\n",
    "    # Define the function that calls the model\n",
    "    def call_model(\n",
    "        state: AgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(AgentState)\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        # First, we define the start node. We use agent.\n",
    "        # This means these are the edges taken after the agent node is called.\n",
    "        \"agent\",\n",
    "        # Next, we pass in the function that will determine which node is called next.\n",
    "        should_continue,\n",
    "        # The mapping below will be used to determine which node to go to\n",
    "        {\n",
    "            # If tools, then we call the tool node.\n",
    "            \"continue\": \"tools\",\n",
    "            # END is a special node marking that the graph should finish.\n",
    "            \"end\": END,\n",
    "        },\n",
    "    )\n",
    "    # We now add a unconditional edge from tools to agent.\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f0bff32-f46e-4c09-810a-4b01a9f1f801",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark-bf16acc8-b3a6-47ca-88aa-62/.ipykernel/17189/command-3951031057998024-3968363664:15: FutureWarning: ``mlflow.langchain.output_parsers.ChatCompletionsOutputParser`` is deprecated. This method will be removed in a future release. Use ``mlflow.langchain.output_parser.ChatCompletionOutputParser`` instead.\n  agent = agent_with_raw_output | RunnableGenerator(wrap_output) | ChatCompletionsOutputParser()\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableGenerator\n",
    "from mlflow.langchain.output_parsers import ChatCompletionsOutputParser\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Create the agent with the system message if it exists\n",
    "try:\n",
    "    agent_prompt = config.get(\"agent_prompt\")\n",
    "    agent_with_raw_output = create_tool_calling_agent(\n",
    "        llm, tools, agent_prompt=agent_prompt\n",
    "    )\n",
    "    display(Image(agent_with_raw_output.get_graph().draw_mermaid_png()))\n",
    "\n",
    "except KeyError:\n",
    "    agent_with_raw_output = create_tool_calling_agent(llm, tools)\n",
    "agent = agent_with_raw_output | RunnableGenerator(wrap_output) | ChatCompletionsOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5902295f-c67b-47ec-923a-4feb7043a854",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test the agent\n",
    "\n",
    "Interact with the agent to test its output. Since this notebook called `mlflow.langchain.autolog()` you can view the trace for each step the agent takes.\n",
    "\n",
    "Replace this placeholder input with an appropriate domain-specific example for your agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42253250-f6dc-489a-aa83-dc486214545c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-bf16acc8-b3a6-47ca-88aa-62fbe52d30f7/lib/python3.10/site-packages/mlflow/langchain/output_parsers.py:46: FutureWarning: ``mlflow.models.rag_signatures.Message`` is deprecated. This method will be removed in a future release. Use ``mlflow.types.llm.ChatMessage`` instead.\n  choices=[ChainCompletionChoice(message=Message(role=\"assistant\", content=text))],\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-bf16acc8-b3a6-47ca-88aa-62fbe52d30f7/lib/python3.10/site-packages/mlflow/langchain/output_parsers.py:46: FutureWarning: ``mlflow.models.rag_signatures.ChainCompletionChoice`` is deprecated. This method will be removed in a future release. Use ``mlflow.types.llm.ChatChoice`` instead.\n  choices=[ChainCompletionChoice(message=Message(role=\"assistant\", content=text))],\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-bf16acc8-b3a6-47ca-88aa-62fbe52d30f7/lib/python3.10/site-packages/mlflow/langchain/output_parsers.py:45: FutureWarning: ``mlflow.models.rag_signatures.ChatCompletionResponse`` is deprecated. This method will be removed in a future release. Use ``mlflow.types.llm.ChatCompletionResponse`` instead.\n  RagChatCompletionResponse(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '<tool_call>{\\n  \"id\": \"call_bdbe14ea-7560-494a-91b0-4af764093ab7\",\\n  \"name\": \"get_weather\",\\n  \"arguments\": \"{\\\\\"__arg1\\\\\": \\\\\"London\\\\\"}\"\\n}</tool_call>\\n\\n'}, 'finish_reason': 'stop'}], 'object': 'chat.completion'} ------------------------------------------------------------\n\novercast clouds\n{'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '<tool_call_result>{\\n  \"id\": \"call_bdbe14ea-7560-494a-91b0-4af764093ab7\",\\n  \"content\": \"overcast clouds\"\\n}</tool_call_result>\\n\\n'}, 'finish_reason': 'stop'}], 'object': 'chat.completion'} ------------------------------------------------------------\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-bf16acc8-b3a6-47ca-88aa-62fbe52d30f7/lib/python3.10/site-packages/mlflow/langchain/output_parsers.py:46: FutureWarning: ``mlflow.models.rag_signatures.Message`` is deprecated. This method will be removed in a future release. Use ``mlflow.types.llm.ChatMessage`` instead.\n  choices=[ChainCompletionChoice(message=Message(role=\"assistant\", content=text))],\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-bf16acc8-b3a6-47ca-88aa-62fbe52d30f7/lib/python3.10/site-packages/mlflow/langchain/output_parsers.py:46: FutureWarning: ``mlflow.models.rag_signatures.ChainCompletionChoice`` is deprecated. This method will be removed in a future release. Use ``mlflow.types.llm.ChatChoice`` instead.\n  choices=[ChainCompletionChoice(message=Message(role=\"assistant\", content=text))],\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-bf16acc8-b3a6-47ca-88aa-62fbe52d30f7/lib/python3.10/site-packages/mlflow/langchain/output_parsers.py:45: FutureWarning: ``mlflow.models.rag_signatures.ChatCompletionResponse`` is deprecated. This method will be removed in a future release. Use ``mlflow.types.llm.ChatCompletionResponse`` instead.\n  RagChatCompletionResponse(\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-bf16acc8-b3a6-47ca-88aa-62fbe52d30f7/lib/python3.10/site-packages/mlflow/langchain/output_parsers.py:46: FutureWarning: ``mlflow.models.rag_signatures.Message`` is deprecated. This method will be removed in a future release. Use ``mlflow.types.llm.ChatMessage`` instead.\n  choices=[ChainCompletionChoice(message=Message(role=\"assistant\", content=text))],\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-bf16acc8-b3a6-47ca-88aa-62fbe52d30f7/lib/python3.10/site-packages/mlflow/langchain/output_parsers.py:46: FutureWarning: ``mlflow.models.rag_signatures.ChainCompletionChoice`` is deprecated. This method will be removed in a future release. Use ``mlflow.types.llm.ChatChoice`` instead.\n  choices=[ChainCompletionChoice(message=Message(role=\"assistant\", content=text))],\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-bf16acc8-b3a6-47ca-88aa-62fbe52d30f7/lib/python3.10/site-packages/mlflow/langchain/output_parsers.py:45: FutureWarning: ``mlflow.models.rag_signatures.ChatCompletionResponse`` is deprecated. This method will be removed in a future release. Use ``mlflow.types.llm.ChatCompletionResponse`` instead.\n  RagChatCompletionResponse(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'The weather in London is overcast clouds.\\n\\n'}, 'finish_reason': 'stop'}], 'object': 'chat.completion'} ------------------------------------------------------------\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-c07403bc5f974d67ab6089435c559748\"",
      "text/plain": [
       "Trace(request_id=tr-c07403bc5f974d67ab6089435c559748)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: replace this placeholder input example with an appropriate domain-specific example for your agent\n",
    "for event in agent.stream({\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in London?\"}]}):\n",
    "    print(event, \"---\" * 20 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89acac5e-0cd9-4de7-b3d7-abb295688aeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.models.set_model(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14b35ec1-3b6a-4c1d-bd86-8f0185b24b59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next steps\n",
    "\n",
    "You can rerun the cells above to iterate and test the agent.\n",
    "\n",
    "Go to the auto-generated [driver]($./driver) notebook in this folder to log, register, and deploy the agent."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}